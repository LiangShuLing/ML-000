{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HomeWork_chap10.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LiangShuLing/ML-000/blob/main/Week13/HomeWork_chap10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm9iNMw5nMtl"
      },
      "source": [
        "# PyTorch Training Short:\n",
        "\n",
        "\n",
        "---\n",
        "First let us check whether the whole thing is working\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CI_nJEend03"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change b type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6u2qdWjdct8"
      },
      "source": [
        "# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZkTfRt8dt2i"
      },
      "source": [
        "!pip install pytorch-lightning sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p3Tbx8cWEFA"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0J_DnaeoBTi"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42H2EckBoKAf"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5MT1Sh1ot60"
      },
      "source": [
        "### Create a simple dataset to test the correctness of our approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vPVo_jIoZ9Y"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "x = torch.randn(100000, 2)\n",
        "noise = torch.randn(100000,)\n",
        "y = ((1.0*x[:,0]+2.0*x[:,1]+noise)>0).type(torch.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz_JAic0o1_L"
      },
      "source": [
        "y_np = y.numpy()\n",
        "x_np = x.numpy()\n",
        "y_train, y_test = y_np[:50000], y_np[50000:]\n",
        "x_train, x_test = x_np[:50000, :], x_np[50000:, :]\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(x_train, y_train)\n",
        "y_pred = log_reg.predict(x_test)\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjdyrSD2pIoX"
      },
      "source": [
        "### Now create an evil data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuI0QJF-o_fe"
      },
      "source": [
        "x_1 = torch.randn(100000)\n",
        "x_2 = torch.randn(100000)\n",
        "x_useful = torch.cos(1.5*x_1)*(x_2**2)\n",
        "x_1_rest_small = torch.randn(100000, 15)+ 0.01*x_1.unsqueeze(1)\n",
        "x_1_rest_large = torch.randn(100000, 15) + 0.1*x_1.unsqueeze(1)\n",
        "x_2_rest_small = torch.randn(100000, 15)+ 0.01*x_2.unsqueeze(1)\n",
        "x_2_rest_large = torch.randn(100000, 15) + 0.1*x_2.unsqueeze(1)\n",
        "x = torch.cat([x_1[:, None], x_2[:, None], x_1_rest_small, x_1_rest_large, x_2_rest_small, x_2_rest_large], dim=1)\n",
        "y = ((10*x_useful) + 5*torch.randn(100000) >0.0).type(torch.int64) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zBxF0QCp-Na"
      },
      "source": [
        "### Now let us test if we have an oracle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3jSEESWp6mN"
      },
      "source": [
        "y_train, y_test = y.numpy()[:50000], y.numpy()[50000:]\n",
        "x_train, x_test = x.numpy()[:50000, :], x.numpy()[50000:, :]\n",
        "oracle_train, oracle_test = x_useful.numpy()[:50000], x_useful.numpy()[50000:]\n",
        "log_reg_2 = LogisticRegression()\n",
        "log_reg_2.fit(oracle_train[:, None],y_train)\n",
        "y_pred = log_reg_2.predict(oracle_test[:, None])\n",
        "print(accuracy_score(y_pred, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUZ94ad7qVhZ"
      },
      "source": [
        "### What if the oracle is not here?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SETnkiN1qTHU"
      },
      "source": [
        "y_train, y_test = y.numpy()[:50000], y.numpy()[50000:]\n",
        "x_train, x_test = x.numpy()[:50000, :], x.numpy()[50000:, :]\n",
        "log_reg_3 = LogisticRegression()\n",
        "log_reg_3.fit(x_train, y_train)\n",
        "y_pred = log_reg_3.predict(x_test)\n",
        "accuracy_score(y_pred, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH7Anlg0qg2j"
      },
      "source": [
        "### Let us run a basic PyTorch example to test whether the good is correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXXQ_Tbzqe8k"
      },
      "source": [
        "x = torch.randn(100000, 2)\n",
        "noise = torch.randn(100000,)\n",
        "y = ((1.0*x[:,0]+2.0*x[:,1]+noise)>0).type(torch.int64)\n",
        "x_train, x_test = x[:50000, :], x[50000:, :]\n",
        "y_train, y_test = y[:50000], y[50000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0R12sOnquZq"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class MyDataSet(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        super().__init__()\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.len = x.shape[0]\n",
        "        \n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx, :], self.y[idx]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnHG9ujxqxZt"
      },
      "source": [
        "train_dataset = MyDataSet(x_train, y_train)\n",
        "test_dataset = MyDataSet(x_test, y_test)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = 128, shuffle=True, num_workers=6)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = 128, num_workers=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO7lkElKrEOO"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def mish(input):\n",
        "\n",
        "    return input * torch.tanh(F.softplus(input))\n",
        "\n",
        "class Mish(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Init method.\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        '''\n",
        "        Forward pass of the function.\n",
        "        '''\n",
        "        return mish(input)\n",
        "\n",
        "class MLPLayer(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, res_coef = 0, dropout_p = 0.1):\n",
        "        super().__init__()\n",
        "        self.linear  = nn.Linear(dim_in, dim_out)\n",
        "        self.res_coef = res_coef\n",
        "        self.activation = Mish()\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.ln = nn.LayerNorm(dim_out)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        y = self.linear(x)\n",
        "        y = self.activation(y)\n",
        "        y = self.dropout(y)\n",
        "        if self.res_coef == 0:\n",
        "            return self.ln(y)\n",
        "        else:\n",
        "            return self.ln(self.res_coef*x +y )\n",
        "\n",
        "       \n",
        "class MyNetwork(nn.Module):\n",
        "    def __init__(self, dim_in, dim, res_coef=0.5, dropout_p = 0.1, n_layers = 10):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.ModuleList()\n",
        "        self.first_linear = MLPLayer(dim_in, dim)\n",
        "        self.n_layers = n_layers\n",
        "        for i in range(n_layers):\n",
        "            self.mlp.append(MLPLayer(dim, dim, res_coef, dropout_p))\n",
        "        self.final = nn.Linear(dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.first_linear(x)\n",
        "        for layer in self.mlp:\n",
        "            x = layer(x)\n",
        "        x= self.sigmoid(self.final(x))\n",
        "        return x.squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ka-3_rfrTKw"
      },
      "source": [
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.metrics import Accuracy\n",
        "class TrainingModule(pl.LightningModule):\n",
        "    def __init__(self, dim_in, dim, res_coef=0, dropout_p=0, n_layers=10):\n",
        "        super().__init__()\n",
        "        self.backbone = MyNetwork(dim_in, dim, res_coef, dropout_p, n_layers)\n",
        "        self.loss = nn.BCELoss()\n",
        "        self.accuracy = Accuracy()\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        x = self.backbone(x)\n",
        "        loss = self.loss(x, y.type(torch.float32))\n",
        "        acc = self.accuracy(x, y)\n",
        "        self.log(\"Validation loss\", loss)\n",
        "        self.log(\"Validation acc\", acc)\n",
        "        return loss, acc\n",
        "        \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        x = self.backbone(x)\n",
        "        loss = self.loss(x, y.type(torch.float32))\n",
        "        acc = self.accuracy(x, y)\n",
        "        self.log(\"Training loss\", loss)\n",
        "        self.log(\"Training acc\", acc)\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer\n",
        "\n",
        "import os\n",
        "class CheckpointEveryNSteps(pl.Callback):\n",
        "    def __init__(self, save_step_frequency):\n",
        "        self.save_step_frequency = save_step_frequency\n",
        "\n",
        "    def on_batch_end(self, trainer: pl.Trainer, _):\n",
        "        epoch = trainer.current_epoch\n",
        "        global_step = trainer.global_step\n",
        "        if global_step % self.save_step_frequency == 0:\n",
        "            filename = \"epoch=\" + str(epoch) + \"_step=\" + str(global_step)+\".ckpt\"\n",
        "            ckpt_path = os.path.join(trainer.checkpoint_callback.dirpath, filename)\n",
        "            trainer.save_checkpoint(ckpt_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47ns8nDyrd3v"
      },
      "source": [
        "from pytorch_lightning import loggers as pl_loggers\n",
        "\n",
        "tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
        "save_by_steps = CheckpointEveryNSteps(100)\n",
        "training_module = TrainingModule(2, 10, 0.5, 0.1, 2)\n",
        "trainer = pl.Trainer(max_epochs=5, gpus=1, progress_bar_refresh_rate=100, val_check_interval=0.25, logger=tb_logger)\n",
        "trainer.fit(training_module, train_dataloader, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9851mNJrlFi"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru23N23JSCNM"
      },
      "source": [
        "\n",
        "### Now Let us use the evil dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQRdDlCTSh9T"
      },
      "source": [
        "import torch\n",
        "x_1 = torch.randn(100000)\n",
        "x_2 = torch.randn(100000)\n",
        "x_useful = torch.cos(1.5*x_1)*(x_2**2)\n",
        "x_1_rest_small = torch.randn(100000, 15)+ 0.01*x_1.unsqueeze(1)\n",
        "x_1_rest_large = torch.randn(100000, 15) + 0.1*x_1.unsqueeze(1)\n",
        "x_2_rest_small = torch.randn(100000, 15)+ 0.01*x_2.unsqueeze(1)\n",
        "x_2_rest_large = torch.randn(100000, 15) + 0.1*x_2.unsqueeze(1)\n",
        "x = torch.cat([x_1[:, None], x_2[:, None], x_1_rest_small, x_1_rest_large, x_2_rest_small, x_2_rest_large], dim=1)\n",
        "y = ((10*x_useful) + 5*torch.randn(100000) >0.0).type(torch.int64) \n",
        "\n",
        "x_train, x_test = x[:50000, :], x[50000:, :]\n",
        "y_train, y_test = y[:50000], y[50000:]\n",
        "train_dataset = MyDataSet(x_train, y_train)\n",
        "test_dataset = MyDataSet(x_test, y_test)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = 32, num_workers=6)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = 128, num_workers=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSR6VqoaS-L9"
      },
      "source": [
        "from pytorch_lightning import loggers as pl_loggers\n",
        "\n",
        "tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
        "\n",
        "training_module = TrainingModule(62, 32, 0.5, 0.1, 20)\n",
        "trainer = pl.Trainer(max_epochs=3, gpus=1, progress_bar_refresh_rate=100, val_check_interval=0.5, logger=tb_logger)\n",
        "trainer.fit(training_module, train_dataloader, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dGk-DQETEs0"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjdSSleYTN3o"
      },
      "source": [
        "Now let us see how LightGBM works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xF66ZMrMTS6X"
      },
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "x_train_np, x_test_np = x_train.numpy(), x_test.numpy()\n",
        "y_train_np, y_test_np = y_train.numpy(), y_test.numpy()\n",
        "\n",
        "train_dataset = lgb.Dataset(x_train_np, y_train_np)\n",
        "test_dataset = lgb.Dataset(x_test_np, y_test_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-pESKZ2UAXC"
      },
      "source": [
        "params = {'num_leaves': 31, 'objective': 'binary', 'feature_fraction':0.8, 'bagging_fraction':0.8, 'metric':'binary_error'}\n",
        "num_round=2000\n",
        "eval_list = [train_dataset, test_dataset]\n",
        "lgb_model = lgb.train(params, train_dataset, num_round, valid_sets=eval_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Orx1fAGVJGj"
      },
      "source": [
        "How can we improve (save) our deep learning model. \n",
        "1. Discretize\n",
        "2. Variable selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSeOMEFlVjUN"
      },
      "source": [
        "# coding = 'utf-8'\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "\n",
        "def encode_label(x):\n",
        "    unique=sorted(list(set([str(item) for item in np.unique(x)])))\n",
        "    kv = {unique[i]: i for i in range(len(unique))}\n",
        "    vfunc = np.vectorize(lambda x: kv[str(x)])\n",
        "    return vfunc(x)\n",
        "\n",
        "def encode_label_mat(x):\n",
        "    _, ncol = x.shape\n",
        "    result = np.empty_like(x, dtype=int)\n",
        "    for col in range(ncol):\n",
        "        result[:,col] = encode_label(x[:, col])\n",
        "    return result\n",
        "\n",
        "def impute_nan(x, method='median'):\n",
        "    _, ncol = x.shape\n",
        "    result = np.empty_like(x)\n",
        "\n",
        "    for col in range(ncol):\n",
        "        if method == 'median':\n",
        "            data = x[:, col]\n",
        "            impute_value = np.median(data[~pd.isnull(data) & (data != np.inf) & (data != -np.inf)])\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        func = np.vectorize(lambda x: impute_value if pd.isnull(x) else x)\n",
        "        result[:, col] = func(x[:, col])\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_uniform_interval(minimum, maximum, nbins):\n",
        "    result = [minimum]\n",
        "    step_size = (float(maximum - minimum)) / nbins\n",
        "    for index in range(nbins - 1):\n",
        "        result.append(minimum + step_size * (index + 1))\n",
        "    result.append(maximum)\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_interval_v2(x, sorted_intervals):\n",
        "    if pd.isnull(x):\n",
        "        return -1\n",
        "    if x == np.inf:\n",
        "        return -2\n",
        "    if x == -np.inf:\n",
        "        return -3\n",
        "    interval = 0\n",
        "    found = False\n",
        "    sorted_intervals.append(np.inf)\n",
        "    while not found and interval < len(sorted_intervals) - 1:\n",
        "        if sorted_intervals[interval] <= x < sorted_intervals[interval + 1]:\n",
        "            return interval\n",
        "        else:\n",
        "            interval += 1\n",
        "\n",
        "\n",
        "def get_quantile_interval(data, nbins):\n",
        "    quantiles = get_uniform_interval(0, 1, nbins)\n",
        "    return list(np.quantile(data[(~pd.isnull(data)) & (data != np.inf) & (data != -np.inf)], quantiles))\n",
        "\n",
        "\n",
        "def discretize(x, nbins=20):\n",
        "    nrow, ncol = x.shape\n",
        "    result = np.empty_like(x)\n",
        "    interval_list = list()\n",
        "    for col in range(ncol):\n",
        "        intervals = sorted(list(set(get_quantile_interval(x[:, col], nbins))))\n",
        "        interval_centroid = list()\n",
        "\n",
        "        for i in range(len(intervals) - 1):\n",
        "            interval_centroid.append(0.5 * (intervals[i] + intervals[i + 1]))\n",
        "        func = np.vectorize(lambda x: get_interval_v2(x, intervals))\n",
        "        result[:, col] = encode_label(func(x[:, col]))\n",
        "        interval_list.append(interval_centroid)\n",
        "    return result.astype(np.int64), interval_list\n",
        "\n",
        "def get_var_type(df):\n",
        "    columns = df.columns\n",
        "    continuous_vars = [x for x in columns if x.startswith('continuous_')]\n",
        "    discrete_vars = [x for x in columns if x.startswith('discrete_')]\n",
        "    other_vars = list()\n",
        "    for column in columns:\n",
        "        if column not in continuous_vars and column not in discrete_vars:\n",
        "            other_vars.append(column)\n",
        "    return {'continuous': continuous_vars,\n",
        "            'discrete': discrete_vars,\n",
        "            'other': other_vars}\n",
        "\n",
        "\n",
        "def get_cont_var(df):\n",
        "    var_types = get_var_type(df)\n",
        "    return var_types['continuous']\n",
        "\n",
        "\n",
        "def get_dis_var(df):\n",
        "    var_types = get_var_type(df)\n",
        "    return var_types['discrete']\n",
        "\n",
        "def drop_const_var(data):\n",
        "    result = data.copy(deep=True)\n",
        "    for col in data.columns:\n",
        "        if len(data.loc[~pd.isnull(data[col]), col].unique()) <= 1:\n",
        "            result.drop(columns=col, inplace=True)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-nHUa7FYU8G"
      },
      "source": [
        "x_train_np, x_test_np = x_train.numpy(), x_test.numpy()\n",
        "y_train_np, y_test_np = y_train.numpy(), y_test.numpy()\n",
        "x = np.concatenate([x_train_np, x_test_np])\n",
        "x_dis, centroid = discretize(x)\n",
        "x_dis_train = x_dis[:50000, :]\n",
        "x_dis_test = x_dis[50000:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WViXd9Ldc_Uy"
      },
      "source": [
        "class TabDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        super().__init__()\n",
        "        self.x = torch.from_numpy(x).type(torch.int32) \n",
        "        self.y = torch.from_numpy(y).type(torch.int32)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx, :], self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWljva98c_Uy"
      },
      "source": [
        "### Create Embedding Factories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVX2rMNMc_Uy"
      },
      "source": [
        "!pip install einops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cXlyVDEchPe"
      },
      "source": [
        "\n",
        "class EmbeddingFactory(nn.Module):\n",
        "    def __init__(self, x, dim_out):\n",
        "        super().__init__()\n",
        "        self.dim_out = dim_out\n",
        "        self.module_list = nn.ModuleList(\n",
        "            [nn.Embedding(len(set(np.unique(x[:, col]))), dim_out) for col in range(x.shape[1])])\n",
        "\n",
        "    def forward(self, x):\n",
        "        result = [self.module_list[col](x[:, col]).unsqueeze(2) for col in range(x.shape[1])]\n",
        "        return torch.cat(result, dim=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_LgwyAjc_Uz"
      },
      "source": [
        "from einops import rearrange, reduce, repeat\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.metrics import Accuracy\n",
        "x_dis_test.shape\n",
        "train_dataloader = DataLoader(TabDataset(x_dis_train, y_train_np), batch_size = 32, num_workers=6)\n",
        "test_dataloader = DataLoader(TabDataset(x_dis_test, y_test_np), batch_size = 128, num_workers=6)\n",
        "\n",
        "class TrainingModuleV2(pl.LightningModule):\n",
        "    def __init__(self, x, dim_emb, dim_mlp, res_coef=0, dropout_p=0, n_layers=10):\n",
        "        super().__init__()\n",
        "        self.embedding = EmbeddingFactory(x, dim_emb)\n",
        "        self.backbone = MyNetwork(x.shape[1]*dim_emb, dim_mlp, res_coef, dropout_p, n_layers)\n",
        "        self.loss = nn.BCELoss()\n",
        "        self.accuracy = Accuracy()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        print('End embedding',x.shape)\n",
        "        return self.backbone(x)\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        x = self.embedding(x)\n",
        "        x = rearrange(x, \"b h e -> b (h e)\")\n",
        "        x = self.backbone(x)\n",
        "        loss = self.loss(x, y.type(torch.float32))\n",
        "        acc = self.accuracy(x, y)\n",
        "        self.log(\"Validation loss\", loss)\n",
        "        self.log(\"Validation acc\", acc)\n",
        "        return loss, acc\n",
        "        \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        x = self.embedding(x)   # shape (32,62) to (32,16,62)\n",
        "        x = rearrange(x, \"b h e -> b (h e)\") # shape (32,16,62) to (32,62,12)\n",
        "        x = self.backbone(x)\n",
        "        loss = self.loss(x, y.type(torch.float32))\n",
        "        acc = self.accuracy(x, y)\n",
        "        self.log(\"Training loss\", loss)\n",
        "        self.log(\"Training acc\", acc)\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpCnDu0Hc_Uz"
      },
      "source": [
        "from pytorch_lightning import loggers as pl_loggers\n",
        "\n",
        "tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
        "training_module = TrainingModuleV2(x_dis, 16, 64, 0.5, 0.1, 10)\n",
        "trainer = pl.Trainer(max_epochs=3, gpus=1, progress_bar_refresh_rate=100, val_check_interval=0.5, logger=tb_logger)\n",
        "trainer.fit(training_module, train_dataloader, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y66ZQw5Ec_Uz"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfBt32oNc_U0"
      },
      "source": [
        "### Let us try TabNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-zqa6BLLtq8"
      },
      "source": [
        "!pip install pytorch_tabnet\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNZfWXs0L6dR"
      },
      "source": [
        "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
        "\n",
        "tabnet_clf=TabNetClassifier()\n",
        "tabnet_clf.fit(x_dis_train, y_train_np,\n",
        "               max_epochs=30,\n",
        "               eval_set=[(x_dis_train, y_train_np)],\n",
        "               eval_name=['train'],\n",
        "               eval_metric=['auc'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KeFKJ0fExlg"
      },
      "source": [
        "The train acc is 0.727 now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qgQ4I25NLse"
      },
      "source": [
        "tabnet_clf2=TabNetClassifier(mask_type='entmax')\n",
        "tabnet_clf2.fit(x_dis_train, y_train_np,\n",
        "               max_epochs=30,\n",
        "               eval_set=[(x_dis_train, y_train_np)],\n",
        "               eval_name=['train'],\n",
        "               eval_metric=['auc'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPWT6xyCSK2O"
      },
      "source": [
        "We can see the train acc is up to 0.802\n",
        "\n",
        "\n",
        "**how to add the pretrain into Tabnet?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcML8LPkKb3I"
      },
      "source": [
        "\n",
        "tabnet_clf3=TabNetClassifier(mask_type='entmax',lambda_sparse=0.001,n_a=10)\n",
        "tabnet_clf3.fit(x_dis_train, y_train_np,\n",
        "               max_epochs=30,\n",
        "               eval_set=[(x_dis_train, y_train_np)],\n",
        "               eval_name=['train'],\n",
        "               eval_metric=['auc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4wrcCyVN46v"
      },
      "source": [
        "The train acc up to 0.816"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bvMirildg6l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}